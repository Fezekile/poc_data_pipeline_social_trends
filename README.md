# POC: Batch + Streaming Data Pipeline (Containerized)

This compact POC ingests **batch data** (UN Sanctions XLSX from FIC) and **streaming data** (Twitter/X API v2 Sampled Stream), 
processes/cleans the batch data, **stores streaming tweets to Parquet** and/or **publishes to Kafka**, 
and uses a lightweight **scheduler** to run the batch job daily.

## Services
- **pipeline**: Python app running:
  - A daily-scheduled **batch job** (downloads & cleans sanctions list).
  - A **streaming job** (Twitter/X sampled stream) with reconnection and error handling.
- **redpanda**: Kafka-compatible broker (single node) for a simple streaming sink.

## Quick Start

1. **Set your environment variables** (at least the bearer token for X API):
   ```bash
   export TWITTER_BEARER="YOUR_X_API_BEARER_TOKEN"
   # Optional Kafka sink
   export USE_KAFKA="true"            # "true" or "false"
   export KAFKA_BROKER="redpanda:9092"
   export KAFKA_TOPIC="tweets"
   ```

2. **Build & run** with Docker Compose:
   ```bash
   docker compose up --build
   ```

3. **Outputs**:
   - Batch parquet: `./data/bronze/sanctions_YYYY-MM-DD.parquet`
   - Cleaned parquet: `./data/silver/sanctions_clean.parquet`
   - Streaming parquet files (rolling): `./data/stream/*.parquet`
   - (Optional) Tweets in Kafka topic: `${KAFKA_TOPIC}`

## Stopping
```bash
docker compose down
```

## Notes
- Scheduler uses APScheduler (inside the `pipeline` container). Default: daily at 02:00 UTC.
- Streaming runs continuously and auto-reconnects with exponential backoff and jitter.
- Redpanda is used for a compact, single-binary Kafka alternative (Kafka API compatible).

## Structure
```
poc-data-pipeline/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ entrypoint.sh
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ batch_ingest.py
â”‚   â”œâ”€â”€ stream_ingest.py
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ processing/
â”‚   â””â”€â”€ clean_sanctions.py
â”œâ”€â”€ scheduler/
â”‚   â””â”€â”€ run.py
â””â”€â”€ data/
    â”œâ”€â”€ bronze/
    â”œâ”€â”€ silver/
    â””â”€â”€ stream/
```


## ğŸ—ƒï¸ Lightweight Local Warehouse (DuckDB)
This POC includes a local DuckDB database under `./data/warehouse/warehouse.duckdb`:

- Views over parquet are created/updated by the scheduler step `warehouse/load_to_duckdb.py`:
  - `silver.sanctions` â†’ `./data/silver/sanctions_clean.parquet`
  - `bronze.tweets_raw` â†’ `./data/stream/*.parquet`
  - `curated.tweets` and `curated.tweet_counts_by_lang` (example curated views)

Run ad-hoc queries with:
```bash
docker compose exec pipeline python /app/warehouse/query_examples.py
```
